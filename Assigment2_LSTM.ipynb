{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaisbeham/Applied_deep_learning/blob/main/Assigment2_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM - Fake News detection\n",
        "\n",
        "Fake - 0\n",
        "Real - 1"
      ],
      "metadata": {
        "id": "OzFQgCpKtafy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vvy4rxvLKY6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfzdqQ4HKfSE",
        "outputId": "cd816d9d-1878-4736-a492-6f7d0507f5ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "#from utils import merge_datasets, smaller_set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from matplotlib import pyplot as plt\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding,Dense,LSTM,Dropout,Flatten,BatchNormalization,Conv1D,GlobalMaxPooling1D,MaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.optimizers import  SGD\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#from hyperas.distributions import uniform\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import regularizers\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing import sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "91HtHS8vKXCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "ZeEUBxOQsDWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_news(path, column_text_name):\n",
        "  news = pd.read_csv(path)\n",
        "  news[column_text_name] = news[column_text_name].astype(str)\n",
        "  if column_text_name != \"text\":\n",
        "    news.rename(columns = {column_text_name : \"text\"}, inplace = True)\n",
        "\n",
        "  return news\n",
        "\n",
        "def merge_split_datasets(real, fake, n_testsize = 0.15):\n",
        "  # take 13000 rows from real data\n",
        "  #real = real.sample(n = 13000, random_state= 2, axis = 0)\n",
        "\n",
        "  # add column label\n",
        "  real['label'] = 1\n",
        "  fake['label'] = 0\n",
        "\n",
        "  real = real[['text', 'label']]\n",
        "  fake = fake[['text', 'label']]\n",
        "  \n",
        "\n",
        "  #merge real and fake\n",
        "  # use only the sentence column on fake\n",
        "  merged = pd.DataFrame(real.append(fake, ignore_index = True))\n",
        "  merged = merged.dropna()\n",
        "   \n",
        "  merged = merged.sample(frac = 1, random_state= 1, ignore_index = True)#.reset_index()\n",
        "\n",
        "  merged[\"text\"].dropna(inplace=True)\n",
        "\n",
        "  # convert the column to string\n",
        "  merged['text'] = merged['text'].astype(str)\n",
        "\n",
        "  # apply the `word_tokenize()` function\n",
        "  tokens = merged['text'].apply(word_tokenize)\n",
        "  tokens = tokens.astype(str)\n",
        "  tokens = tokens.apply(str)\n",
        "  #tokens = [w.apply(str) for w in tokens]\n",
        "  #merged['text']=merged['text'].fillna('').apply(str)\n",
        "  print('the type is..', type(tokens))  \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  print(tokens)\n",
        "  #word_tokens = word_tokenize(tokens)\n",
        "  \n",
        "  filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(filtered_sentence,merged['label'], test_size = n_testsize, random_state = 1)\n",
        "\n",
        "  #X_test = test['text']\n",
        "  #Y_test = test['label']\n",
        "\n",
        "  #create validation set\n",
        "  #X = train['text']\n",
        "  #Y = train['label'].astype(int)    \n",
        "  X_train,X_valid,Y_train,Y_valid = train_test_split(X_train,Y_train,test_size=0.15, random_state=42)\n",
        "  \n",
        "  return X_train, Y_train, X_valid, Y_valid, X_test, Y_test\n",
        "\n",
        "\n",
        "def RNN(max_tokens, max_len, dropout):\n",
        "  inputs = Input(name='inputs',shape=[max_len])\n",
        "  layer = Embedding(max_tokens,50,input_length=max_len)(inputs)\n",
        "  layer = LSTM(64)(layer)\n",
        "  layer = Dense(256,name='FC1')(layer)\n",
        "  layer = Activation('relu')(layer)\n",
        "  layer = Dropout(dropout)(layer)\n",
        "  layer = Dense(1,name='out_layer')(layer)\n",
        "  layer = Activation('sigmoid')(layer)\n",
        "  model = Model(inputs=inputs,outputs=layer)\n",
        "  return model\n",
        "\n",
        "#def LSTM(embedding_vector_features=45):\n",
        "    \n",
        " # model=Sequential()\n",
        "\n",
        "  #model.add(Embedding(max_words,embedding_vector_features,input_length=max_len))\n",
        "\n",
        "  #model.add(LSTM(128,activation='relu',return_sequences=True))\n",
        "\n",
        "  #model.add(Dropout(0.2))\n",
        "\n",
        "  #model.add(LSTM(128,activation='relu'))\n",
        "\n",
        "  #model.add(Dropout(0.2))\n",
        "\n",
        "  # for units in [128,128,64,32]:\n",
        "\n",
        "  # model.add(Dense(units,activation='relu'))\n",
        "\n",
        "  # model.add(Dropout(0.2))\n",
        "\n",
        "  #model.add(Dense(32,activation='relu'))\n",
        "\n",
        "  #model.add(Dropout(0.2))\n",
        "\n",
        "  #model.add(Dense(4,activation='softmax'))\n",
        "  #return model\n",
        "\n",
        "def predict_result(X_test, Y_test, tok):\n",
        "  test_sequences = tok.texts_to_sequences(X_test)\n",
        "  test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "  accr = model.evaluate(test_sequences_matrix,Y_test)\n",
        "  print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "  predict_labels = model.predict(test_sequences_matrix) \n",
        "\n",
        "  # convert the range predicition into 0s or 1s\n",
        "  Y_pred =[ 1 if i>0.5 else 0 for i in predict_labels  ] \n",
        "\n",
        "  print(\"Number Real news:\",sum(Y_pred))\n",
        "  print(\"Number Fake news:\",len(Y_pred)-sum(Y_pred))\n",
        "  print()\n",
        "  print(\"Classification Report\")\n",
        "  print(classification_report(Y_test, Y_pred, target_names = ['class 0', 'class 1']))\n",
        "  print()\n",
        "  print(\"Confusion Matrix\")\n",
        "  matrix=confusion_matrix(Y_test,Y_pred,labels=[0,1])\n",
        "  cm=pd.DataFrame(matrix,index=['class_0 pred','class_1 pred'],columns=['class_0 True','class_1 True'])\n",
        "  print(cm)\n",
        "  return\n",
        "\n",
        "def trainer(X_train, Y_train, max_words, max_len, n_batchsize, n_epochs, model):\n",
        "  tok = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "  tok.fit_on_texts(X_train)\n",
        "  sequences = tok.texts_to_sequences(X_train)\n",
        "  sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "  print('Found %s unique tokens.' % len(sequences_matrix))\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "  model.fit(sequences_matrix,Y_train,batch_size=n_batchsize,epochs=n_epochs,\n",
        "          validation_split=0.2)\n",
        "  \n",
        "  return model, tok\n",
        "\n"
      ],
      "metadata": {
        "id": "JKzSXRq2lSJ4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read datasets"
      ],
      "metadata": {
        "id": "9ceouWvH4YrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read datasets\n",
        "fake_news1 = read_news(\"/content/drive/MyDrive/dados_/fake.csv\", \"text\")\n",
        "fake_news2 = read_news(\"/content/drive/MyDrive/dados_/Fake_bisaillon.csv\", \"text\")\n",
        "\n",
        "real_news1 = read_news(\"/content/drive/MyDrive/dados_/df_2016.csv\", \"sentence\")\n",
        "real_news2 = read_news(\"/content/drive/MyDrive/dados_/True_bisaillon.csv\", \"text\")\n",
        "\n",
        "#shape of each dataset\n",
        "print(\"fake_news1: \", np.shape(fake_news1), \"\\nfake_news2: \",  np.shape(fake_news2), \"\\nreal_news1: \",  np.shape(real_news1),\"\\nreal_news2: \", np.shape(real_news2)  )\n",
        "\n",
        "real_news1_smallset = real_news1.sample(n = 15000, random_state= 2, axis = 0)\n",
        "\n",
        "print(\"real_news1 small set:\", np.shape(real_news1_smallset))\n",
        "# separate the location of the news from the news text (e.g. \"WASHINGTON (Reuters)\")\n",
        "real_news2[[\"loc\", \"text\"]] = real_news2['text'].str.split('-',1, expand = True)\n",
        "#real_news2.head()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv6G6AP6TXRN",
        "outputId": "21c8eebb-e1c5-47b5-9e60-7ea61253242d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fake_news1:  (12999, 20) \n",
            "fake_news2:  (23481, 4) \n",
            "real_news1:  (105606, 3) \n",
            "real_news2:  (21417, 4)\n",
            "real_news1 small set: (15000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train model with 2 datasets with similar lenght**\n",
        "> Dataset fake.csv and df_2016.csv"
      ],
      "metadata": {
        "id": "f8h7IHbTMNeK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xdml4194nYuk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = merge_split_datasets(real_news1, fake_news1)"
      ],
      "metadata": {
        "id": "whp8fIVsMMpS",
        "outputId": "3b2f3606-f841-45ee-f73d-0050e721d4d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the type is.. <class 'pandas.core.series.Series'>\n",
            "0         ['Elsa', 'Dorfman', 'turned', 'one', 'of', 'th...\n",
            "1         ['Although', 'it', 'had', 'been', 'alerted', '...\n",
            "2         ['It', '’', 's', 'not', 'hard', 'to', 'see', '...\n",
            "3         ['Hackers', 'exposed', 'Clinton', 'as', 'a', '...\n",
            "4         ['A', 'critical', 'guide', 'to', 'exhibitions'...\n",
            "                                ...                        \n",
            "118600    ['A', 'Federal', 'appeals', 'court', 'ruled', ...\n",
            "118601    ['A', 'longtime', 'travel', 'writer', 'shares'...\n",
            "118602    ['Vehicles', 'are', 'increasingly', 'able', 't...\n",
            "118603    ['Old-fashioned', 'gets', 'radical', 'in', 'a'...\n",
            "118604    ['After', 'a', '2-3', 'start', ',', 'Ben', 'Mc...\n",
            "Name: text, Length: 118605, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters tunning"
      ],
      "metadata": {
        "id": "e6gKmUdZMMaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000\n",
        "max_len = 300\n",
        "n_batchsize = 128\n",
        "n_epochs = 10\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "KwfRpNEQNXRk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "i4NPOOV8C3f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "def get_frequency(text):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        #text = tuple\n",
        "        #text = [x for x in tuple]\n",
        "        text = ''.join(text)\n",
        "        trueCauseWords = nltk.word_tokenize(text)\n",
        "        stopword = set(stopwords.words(\"english\"))\n",
        "        words = [\n",
        "                lemmatizer.lemmatize(word).lower()\n",
        "                for word in trueCauseWords\n",
        "                if re.match(\"\\w\", word) and (word.lower() not in stopword)\n",
        "                ]\n",
        "        #trueCauseWords = [word for line in trueCauseText for word in line.split()]\n",
        "        counts = Counter( x for x in words)\n",
        "        print(counts.most_common(15))\n",
        "        return"
      ],
      "metadata": {
        "id": "7sxFOTYkCftg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_frequency(X_train)"
      ],
      "metadata": {
        "id": "WUhlVll3Ch43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "Q5DuJkpANjBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(max_tokens = max_words, max_len = max_len, dropout = dropout )\n",
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')\n",
        "trained_model, tok = trainer(X_train, Y_train, max_words = max_words, max_len = max_len, n_batchsize = n_batchsize, n_epochs = n_epochs, model = model)"
      ],
      "metadata": {
        "id": "SeHUxroRNdJk",
        "outputId": "9ca8686e-fcfb-4391-8c67-a2bd476eb181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 85691 unique tokens.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputs (InputLayer)         [(None, 300)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 300, 50)           50000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                29440     \n",
            "                                                                 \n",
            " FC1 (Dense)                 (None, 256)               16640     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " out_layer (Dense)           (None, 1)                 257       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,337\n",
            "Trainable params: 96,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "536/536 [==============================] - 18s 20ms/step - loss: 0.0680 - accuracy: 0.9806 - val_loss: 0.0317 - val_accuracy: 0.9898\n",
            "Epoch 2/10\n",
            "536/536 [==============================] - 12s 21ms/step - loss: 0.0312 - accuracy: 0.9897 - val_loss: 0.0284 - val_accuracy: 0.9902\n",
            "Epoch 3/10\n",
            "536/536 [==============================] - 11s 21ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.0259 - val_accuracy: 0.9916\n",
            "Epoch 4/10\n",
            "536/536 [==============================] - 10s 19ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0252 - val_accuracy: 0.9922\n",
            "Epoch 5/10\n",
            "536/536 [==============================] - 10s 18ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.0291 - val_accuracy: 0.9910\n",
            "Epoch 6/10\n",
            "536/536 [==============================] - 10s 19ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.0227 - val_accuracy: 0.9926\n",
            "Epoch 7/10\n",
            "536/536 [==============================] - 10s 19ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.0222 - val_accuracy: 0.9931\n",
            "Epoch 8/10\n",
            "536/536 [==============================] - 11s 20ms/step - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.0224 - val_accuracy: 0.9935\n",
            "Epoch 9/10\n",
            "536/536 [==============================] - 10s 19ms/step - loss: 0.0130 - accuracy: 0.9951 - val_loss: 0.0239 - val_accuracy: 0.9926\n",
            "Epoch 10/10\n",
            "536/536 [==============================] - 10s 19ms/step - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.0289 - val_accuracy: 0.9933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict - Valid set"
      ],
      "metadata": {
        "id": "AYfBMgFKOMvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result(X_valid, Y_valid, tok)"
      ],
      "metadata": {
        "id": "hd6gYeY-OMeW",
        "outputId": "0b2d86ba-73ce-4877-ee17-9a94132566a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "473/473 [==============================] - 3s 6ms/step - loss: 0.0257 - accuracy: 0.9934\n",
            "Test set\n",
            "  Loss: 0.026\n",
            "  Accuracy: 0.993\n",
            "473/473 [==============================] - 3s 5ms/step\n",
            "Number Real news: 13496\n",
            "Number Fake news: 1627\n",
            "\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.98      0.96      0.97      1665\n",
            "     class 1       0.99      1.00      1.00     13458\n",
            "\n",
            "    accuracy                           0.99     15123\n",
            "   macro avg       0.99      0.98      0.98     15123\n",
            "weighted avg       0.99      0.99      0.99     15123\n",
            "\n",
            "\n",
            "Confusion Matrix\n",
            "              class_0 True  class_1 True\n",
            "class_0 pred          1596            69\n",
            "class_1 pred            31         13427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Test other parameters\n",
        "\n",
        "max_words = 2000\n",
        "max_len = 500 \n",
        "n_batchsize = 128\n",
        "n_epochs = 10\n",
        "dropout = 0.2\n",
        "\n",
        "model2 = RNN(max_tokens = max_words, max_len = max_len, dropout = dropout )\n",
        "\n",
        "trained_model2, tok = trainer(X_train, Y_train, max_words = max_words, max_len = max_len, n_batchsize = n_batchsize, n_epochs = n_epochs, model = model2)\n",
        "predict_result(X_valid, Y_valid, tok)"
      ],
      "metadata": {
        "id": "nmelI82nUFtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second parameter test has higher valeus for vocabulary size ( max_words) and lenght of sentences (max_len). This choice resulted in even higher accuracy, which could indicate overfit of the data, therefore the previous parameters were the one choosen."
      ],
      "metadata": {
        "id": "EhxOkkiUWbE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test other model\n",
        "https://www.kaggle.com/code/khotijahs1/using-lstm-for-nlp-text-classification\n",
        "model2 = LSTM()\n",
        "\n",
        "trained_model2, tok = trainer(X_train, Y_train, max_words = max_words, max_len = max_len, n_batchsize = n_batchsize, n_epochs = n_epochs, model = model2)\n",
        "predict_result(X_valid, Y_valid, tok)"
      ],
      "metadata": {
        "id": "WT3gCqhwbCyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parmeters = {max_words = [500, 1000, 2000],\n",
        "            max_len = [100, 500, 1000],\n",
        "            dropout = [0.1, 0.2, 0.5] }\n",
        "def CVsearch(parameters, model):\n",
        "  max_words = [parameter[0] for parameter in parameters]\n",
        "  max_len =  [parameter[1] for parameter in parameters]\n",
        "  dropout = [parameter[2] for parameter in parameters]\n",
        "\n",
        "  optimal_fscore = 0  \n",
        "  optimal_params = [0,0,0,0]\n",
        "\n",
        "  for i in max_words:\n",
        "    for j in max_len:\n",
        "      for k in dropout:\n",
        "\n",
        "        print(\"checking parameters\", output_dim,input_dim, emb, hidd)\n",
        "        kf = k_fold(n_splits=folds)\n",
        "        indices = kf.split()\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hnOT7mq1Xw3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict - Test set"
      ],
      "metadata": {
        "id": "CsaqkgDsOLwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result(X_test, Y_test, tok)"
      ],
      "metadata": {
        "id": "CAC_5uWpOLdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train model with 2 datasets**\n",
        "\n",
        "> Dataset fake.csv and df_2016.csv\n"
      ],
      "metadata": {
        "id": "UBSus53f72RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = merge_split_datasets(real_news1, fake_news1)"
      ],
      "metadata": {
        "id": "IUb82hpo71ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters tunning"
      ],
      "metadata": {
        "id": "PhbcwCGj83Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000\n",
        "max_len = 150\n",
        "n_batchsize = 128\n",
        "n_epochs = 10"
      ],
      "metadata": {
        "id": "DYSQnx5k86_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "1nTIOgOY9In6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()\n",
        "\n",
        "trained_model, tok = trainer(X_train, Y_train, max_words = max_words, max_len = max_len, n_batchsize = n_batchsize, n_epochs = n_epochs, model = model)"
      ],
      "metadata": {
        "id": "o1jt7DSY9IR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict - Valid set"
      ],
      "metadata": {
        "id": "fPD5h7X5-Lbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result(X_valid, Y_valid, tok)"
      ],
      "metadata": {
        "id": "bQ-1RJsU-MB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict - Test set"
      ],
      "metadata": {
        "id": "LP0MiIH0-Mf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result(X_test, Y_test, tok)"
      ],
      "metadata": {
        "id": "dYETNYTv-NDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train model with  4 datasets**\n",
        "\n",
        "> Datasets: fake.csv, df_2016.csv, fake_Bisallion.csv and True_Bisallion.csv"
      ],
      "metadata": {
        "id": "UdvQVk6Y7xZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "real_news = pd.DataFrame(real_news1[\"text\"].append(real_news2[\"text\"], ignore_index = True))\n",
        "fake_news = pd.DataFrame(fake_news1[\"text\"].append(fake_news2[\"text\"], ignore_index = True))\n",
        "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = merge_split_datasets(real_news, fake_news)"
      ],
      "metadata": {
        "id": "B092VQVap4te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters tunning"
      ],
      "metadata": {
        "id": "sM0R6v1j4npk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 1000\n",
        "max_len = 150\n",
        "n_batchsize = 128\n",
        "n_epochs = 10\n"
      ],
      "metadata": {
        "id": "3QxJV4OtW42Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train model"
      ],
      "metadata": {
        "id": "K3np9KxP4wbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()\n",
        "\n",
        "trained_model, tok = trainer(X_train, Y_train, max_words = max_words, max_len = max_len, n_batchsize = n_batchsize, n_epochs = n_epochs, model = model)"
      ],
      "metadata": {
        "id": "QwChq8wbW_bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict - Valid set"
      ],
      "metadata": {
        "id": "zkla5Kgz41cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result(X_valid, Y_valid, tok)"
      ],
      "metadata": {
        "id": "3wSmSyeTxCPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict - test set"
      ],
      "metadata": {
        "id": "1aH96O7D46bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result(X_test, Y_test, tok)"
      ],
      "metadata": {
        "id": "dV50mB5euziX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}